{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# New Recruit Ladder Scraper for Warhammer The Old World\n",
    "\n",
    "This notebook scrapes army compositions and match data from the New Recruit ladder system for Warhammer The Old World.\n",
    "\n",
    "## Goal\n",
    "Extract the best army compositions for each faction, including:\n",
    "- **Army Composition Data**: Factions, units, numbers, upgrades, equipment, point costs\n",
    "- **Game Outcome Data**: Win/loss records, tournament standings, scores\n",
    "- **Contextual Data**: Scenarios, opponent factions, match details\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Configuration and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base URL: https://www.newrecruit.eu\n",
      "Ladder URL: https://www.newrecruit.eu/ladder\n",
      "Target Game System: Warhammer The Old World\n"
     ]
    }
   ],
   "source": [
    "# Base URLs and configuration\n",
    "BASE_URL = \"https://www.newrecruit.eu\"\n",
    "LADDER_URL = f\"{BASE_URL}/ladder\"\n",
    "\n",
    "# Headers to avoid being blocked\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Rate limiting - be respectful to the server\n",
    "REQUEST_DELAY = 1  # seconds between requests\n",
    "\n",
    "# Game system identifier (may need to be adjusted based on actual dropdown value)\n",
    "TOW_GAME_SYSTEM = \"Warhammer The Old World\"\n",
    "\n",
    "print(f\"Base URL: {BASE_URL}\")\n",
    "print(f\"Ladder URL: {LADDER_URL}\")\n",
    "print(f\"Target Game System: {TOW_GAME_SYSTEM}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def make_request(url, session=None):\n",
    "    \"\"\"Make a web request with error handling and rate limiting.\"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    \n",
    "    try:\n",
    "        time.sleep(REQUEST_DELAY)  # Rate limiting\n",
    "        response = session.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def safe_extract_text(element, default=\"\"):\n",
    "    \"\"\"Safely extract text from a BeautifulSoup element.\"\"\"\n",
    "    return element.get_text(strip=True) if element else default\n",
    "\n",
    "def extract_id_from_url(url, param_name='id'):\n",
    "    \"\"\"Extract ID parameter from URL.\"\"\"\n",
    "    try:\n",
    "        from urllib.parse import urlparse, parse_qs\n",
    "        parsed = urlparse(url)\n",
    "        query_params = parse_qs(parsed.query)\n",
    "        return query_params.get(param_name, [None])[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Test the utility functions\n",
    "print(\"Utility functions defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Player Discovery Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player discovery function defined!\n"
     ]
    }
   ],
   "source": [
    "def get_player_ids_from_ladder(session=None):\n",
    "    \"\"\"\n",
    "    Get player IDs from the New Recruit ladder for Warhammer The Old World.\n",
    "    \n",
    "    Based on user's navigation:\n",
    "    1. Go to https://www.newrecruit.eu/ladder\n",
    "    2. Select \"Warhammer The Old World\" from dropdown\n",
    "    3. Extract player profile links like /app/Profile?id=65c1ed233771bd78230a7539\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    \n",
    "    logger.info(\"Fetching player IDs from ladder...\")\n",
    "    \n",
    "    response = make_request(LADDER_URL, session)\n",
    "    if not response:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Look for player profile links\n",
    "    # Pattern: <a data-v-e0ae6b26=\"\" class=\"blue\" href=\"/app/Profile?id=65c1ed233771bd78230a7539\">Arvid_dc</a>\n",
    "    player_links = soup.find_all('a', {'class': 'blue', 'href': re.compile(r'/app/Profile\\?id=')})\n",
    "    \n",
    "    player_data = []\n",
    "    for link in player_links:\n",
    "        href = link.get('href', '')\n",
    "        player_name = safe_extract_text(link)\n",
    "        player_id = extract_id_from_url(f\"{BASE_URL}{href}\")\n",
    "        \n",
    "        if player_id and player_name:\n",
    "            player_data.append({\n",
    "                'player_id': player_id,\n",
    "                'player_name': player_name,\n",
    "                'profile_url': f\"{BASE_URL}{href}\"\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"Found {len(player_data)} players\")\n",
    "    return player_data\n",
    "\n",
    "# Test function (commented out to avoid making actual requests during setup)\n",
    "# players = get_player_ids_from_ladder()\n",
    "# print(f\"Function defined. Would find players like: {players[:3] if players else 'None found'}\")\n",
    "print(\"Player discovery function defined!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Match History and Army List Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_history_urls(player_profile_url, session=None):\n",
    "    \"\"\"\n",
    "    Extract match history from a player's profile page.\n",
    "    \n",
    "    Based on user's navigation:\n",
    "    1. Go to player profile\n",
    "    2. Look for Match History section: <h3>Match History <span>(68)</span></h3>\n",
    "    3. Find Match Details links\n",
    "    4. Extract army list URLs from match details\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    \n",
    "    logger.info(f\"Fetching match history from {player_profile_url}\")\n",
    "    \n",
    "    response = make_request(player_profile_url, session)\n",
    "    if not response:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Look for Match History section\n",
    "    # Pattern: <h3 data-v-d3008f2d=\"\" class=\"arrowTitle collapsed titleClickEffect titreCategory\">\n",
    "    #          <img data-v-d3008f2d=\"\" src=\"/assets/icons/right1.png\" class=\"icon arrow\"> \n",
    "    #          Match History <span data-v-5d2bbe3b=\"\" class=\"matchNum\">(68)</span></h3>\n",
    "    \n",
    "    match_history_section = soup.find('h3', string=re.compile(r'Match History'))\n",
    "    if not match_history_section:\n",
    "        logger.warning(f\"No match history section found for {player_profile_url}\")\n",
    "        return []\n",
    "    \n",
    "    # Look for Match Details links\n",
    "    # Pattern: <a data-v-c0edcf0d=\"\" href=\"#\">Match Details</a>\n",
    "    match_detail_links = soup.find_all('a', string=re.compile(r'Match Details'))\n",
    "    \n",
    "    match_urls = []\n",
    "    for link in match_detail_links:\n",
    "        href = link.get('href', '')\n",
    "        if href and href != '#':\n",
    "            full_url = urljoin(player_profile_url, href)\n",
    "            match_urls.append(full_url)\n",
    "    \n",
    "    logger.info(f\"Found {len(match_urls)} match detail links\")\n",
    "    return match_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match detail extraction function defined!\n"
     ]
    }
   ],
   "source": [
    "def extract_army_list_urls_from_match(match_detail_url, session=None):\n",
    "    \"\"\"\n",
    "    Extract army list URLs from a match details page.\n",
    "    \n",
    "    Based on user's navigation:\n",
    "    1. Go to match details page\n",
    "    2. Look for eye icon: <img data-v-983b100b=\"\" src=\"/assets/icons/eye.png\">\n",
    "    3. Extract the army list URL that the eye icon links to\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    \n",
    "    logger.info(f\"Extracting army lists from {match_detail_url}\")\n",
    "    \n",
    "    response = make_request(match_detail_url, session)\n",
    "    if not response:\n",
    "        return {}\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Look for eye icons that lead to army lists\n",
    "    # Pattern: <img data-v-983b100b=\"\" src=\"/assets/icons/eye.png\">\n",
    "    eye_icons = soup.find_all('img', {'src': '/assets/icons/eye.png'})\n",
    "    \n",
    "    army_list_data = {\n",
    "        'player_list_url': None,\n",
    "        'opponent_list_url': None,\n",
    "        'match_result': None,\n",
    "        'opponent_faction': None\n",
    "    }\n",
    "    \n",
    "    # Extract army list URLs from eye icon parent links\n",
    "    for i, eye_icon in enumerate(eye_icons):\n",
    "        parent_link = eye_icon.find_parent('a')\n",
    "        if parent_link:\n",
    "            href = parent_link.get('href', '')\n",
    "            if href and '/app/list/' in href:\n",
    "                full_url = urljoin(match_detail_url, href)\n",
    "                if i == 0:\n",
    "                    army_list_data['player_list_url'] = full_url\n",
    "                elif i == 1:\n",
    "                    army_list_data['opponent_list_url'] = full_url\n",
    "    \n",
    "    # Try to extract match result and opponent info (this will depend on page structure)\n",
    "    # This is a placeholder - actual implementation depends on how match results are displayed\n",
    "    result_elements = soup.find_all(string=re.compile(r'(Win|Loss|Draw|Victory|Defeat)'))\n",
    "    if result_elements:\n",
    "        army_list_data['match_result'] = result_elements[0].strip()\n",
    "    \n",
    "    return army_list_data\n",
    "\n",
    "print(\"Match detail extraction function defined!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Army List Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Army list parsing functions defined!\n"
     ]
    }
   ],
   "source": [
    "def parse_army_list(list_url, session=None):\n",
    "    \"\"\"\n",
    "    Parse an army list page to extract detailed composition data.\n",
    "    \n",
    "    Target data to extract:\n",
    "    - Faction name\n",
    "    - Total points\n",
    "    - Units with quantities, upgrades, equipment\n",
    "    - Character details\n",
    "    - Point costs per unit/upgrade\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    \n",
    "    logger.info(f\"Parsing army list from {list_url}\")\n",
    "    \n",
    "    response = make_request(list_url, session)\n",
    "    if not response:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    army_data = {\n",
    "        'list_url': list_url,\n",
    "        'faction': None,\n",
    "        'list_name': None,\n",
    "        'total_points': None,\n",
    "        'characters': [],\n",
    "        'core_units': [],\n",
    "        'special_units': [],\n",
    "        'rare_units': [],\n",
    "        'raw_html': str(soup)  # For debugging and manual review\n",
    "    }\n",
    "    \n",
    "    # Extract list title and faction\n",
    "    # This will depend on the specific HTML structure of New Recruit list pages\n",
    "    title_element = soup.find('title')\n",
    "    if title_element:\n",
    "        title_text = safe_extract_text(title_element)\n",
    "        army_data['list_name'] = title_text\n",
    "        \n",
    "        # Try to extract faction from title\n",
    "        # Common patterns: \\\"Faction Name - List Name - [Points]\\\"\n",
    "        if ' - ' in title_text:\n",
    "            parts = title_text.split(' - ')\n",
    "            if len(parts) >= 1:\n",
    "                army_data['faction'] = parts[0].strip()\n",
    "    \n",
    "    # Extract total points\n",
    "    # Look for patterns like \\\"[1495pts]\\\" or \\\"1495 points\\\"\n",
    "    points_pattern = re.compile(r'\\\\[?(\\\\d+)\\\\s*pts?\\\\]?', re.IGNORECASE)\n",
    "    page_text = soup.get_text()\n",
    "    points_match = points_pattern.search(page_text)\n",
    "    if points_match:\n",
    "        army_data['total_points'] = int(points_match.group(1))\n",
    "    \n",
    "    # Extract units by category\n",
    "    # This is complex and will depend on New Recruit's HTML structure\n",
    "    # For now, we'll store the raw HTML for manual inspection\n",
    "    \n",
    "    return army_data\n",
    "\n",
    "def extract_units_from_list_section(soup, section_name):\n",
    "    \"\"\"Helper function to extract units from a specific army list section.\"\"\"\n",
    "    units = []\n",
    "    \n",
    "    # Look for section headers like \\\"Characters\\\", \\\"Core\\\", \\\"Special\\\", \\\"Rare\\\"\n",
    "    section_header = soup.find(string=re.compile(section_name, re.IGNORECASE))\n",
    "    if not section_header:\n",
    "        return units\n",
    "    \n",
    "    # Navigate to the parent element and find following units\n",
    "    # This is highly dependent on New Recruit's HTML structure\n",
    "    \n",
    "    return units\n",
    "\n",
    "print(\"Army list parsing functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium-based approach for dynamic content\n",
    "def get_players_with_selenium(headless=True, timeout=10):\n",
    "    \"\"\"Use Selenium to interact with the dynamic ladder page.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        \n",
    "        # Set up Chrome options\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        \n",
    "        # Create driver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        try:\n",
    "            print(\"üöÄ Opening New Recruit ladder page...\")\n",
    "            driver.get(LADDER_URL)\n",
    "            \n",
    "            # Wait for page to load\n",
    "            wait = WebDriverWait(driver, timeout)\n",
    "            \n",
    "            # Look for the game system dropdown\n",
    "            print(\"üîç Looking for game system dropdown...\")\n",
    "            try:\n",
    "                # Try to find dropdown with \"Warhammer The Old World\" option\n",
    "                dropdown_selectors = [\n",
    "                    \"select\",  # Generic select element\n",
    "                    \"[data-v-5065dd38]\",  # From user's HTML snippet\n",
    "                    \".select\",\n",
    "                    \".dropdown\"\n",
    "                ]\n",
    "                \n",
    "                dropdown = None\n",
    "                for selector in dropdown_selectors:\n",
    "                    try:\n",
    "                        dropdown = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
    "                        print(f\"‚úÖ Found dropdown with selector: {selector}\")\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if dropdown:\n",
    "                    # Try to select \"Warhammer The Old World\"\n",
    "                    select = Select(dropdown)\n",
    "                    \n",
    "                    # Print all available options\n",
    "                    options = select.options\n",
    "                    print(f\"üìã Found {len(options)} dropdown options:\")\n",
    "                    for i, option in enumerate(options):\n",
    "                        option_text = option.get_attribute('textContent') or option.text\n",
    "                        print(f\"  {i}: {option_text}\")\n",
    "                        \n",
    "                        # Try to select Old World option\n",
    "                        if 'old world' in option_text.lower():\n",
    "                            print(f\"üéØ Selecting: {option_text}\")\n",
    "                            select.select_by_visible_text(option_text)\n",
    "                            time.sleep(2)  # Wait for content to load\n",
    "                            break\n",
    "                \n",
    "                # Now look for player links\n",
    "                print(\"üîç Looking for player profile links...\")\n",
    "                player_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='Profile']\")\n",
    "                \n",
    "                players = []\n",
    "                for link in player_links:\n",
    "                    href = link.get_attribute('href')\n",
    "                    text = link.text.strip()\n",
    "                    if href and text and 'id=' in href:\n",
    "                        player_id = extract_id_from_url(href)\n",
    "                        if player_id:\n",
    "                            players.append({\n",
    "                                'player_id': player_id,\n",
    "                                'player_name': text,\n",
    "                                'profile_url': href\n",
    "                            })\n",
    "                \n",
    "                print(f\"‚úÖ Found {len(players)} players!\")\n",
    "                return players\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error finding dropdown or players: {e}\")\n",
    "                \n",
    "                # Fallback: Save page source for debugging\n",
    "                with open('selenium_page_source.html', 'w', encoding='utf-8') as f:\n",
    "                    f.write(driver.page_source)\n",
    "                print(\"üíæ Saved page source to selenium_page_source.html\")\n",
    "                \n",
    "                return []\n",
    "        \n",
    "        finally:\n",
    "            driver.quit()\n",
    "            \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Selenium not available: {e}\")\n",
    "        print(\"Install with: pip install selenium webdriver-manager\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Selenium error: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ Selenium scraper function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first test what we're actually getting from the ladder page\n",
    "response = make_request(LADDER_URL)\n",
    "if response:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    print(\"=== PAGE TITLE ===\")\n",
    "    title = soup.find('title')\n",
    "    if title:\n",
    "        print(title.get_text())\n",
    "    \n",
    "    print(\"\\n=== LOOKING FOR GAME SYSTEM DROPDOWN ===\")\n",
    "    # Look for dropdown options or game system selection\n",
    "    options = soup.find_all('option')\n",
    "    for option in options:\n",
    "        text = safe_extract_text(option)\n",
    "        if text:\n",
    "            print(f\"Option: {text}\")\n",
    "            \n",
    "    print(\"\\n=== LOOKING FOR GAME SYSTEM REFERENCES ===\")\n",
    "    # Look for any text containing \"Warhammer The Old World\"\n",
    "    tow_references = soup.find_all(string=re.compile(r'Warhammer.*Old.*World', re.IGNORECASE))\n",
    "    for ref in tow_references[:5]:  # First 5 matches\n",
    "        print(f\"Found: {ref.strip()}\")\n",
    "    \n",
    "    print(\"\\n=== LOOKING FOR PROFILE LINKS ===\")\n",
    "    # Look for any profile links\n",
    "    profile_links = soup.find_all('a', href=re.compile(r'Profile'))\n",
    "    print(f\"Found {len(profile_links)} profile links\")\n",
    "    for link in profile_links[:3]:  # First 3\n",
    "        href = link.get('href', '')\n",
    "        text = safe_extract_text(link)\n",
    "        print(f\"Link: {text} -> {href}\")\n",
    "        \n",
    "    print(\"\\n=== LOOKING FOR DATA ATTRIBUTES ===\")\n",
    "    # Look for Vue.js data attributes that might indicate dynamic content\n",
    "    vue_elements = soup.find_all(attrs={\"data-v-\": True})\n",
    "    print(f\"Found {len(vue_elements)} Vue.js elements\")\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to fetch the ladder page\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the Selenium scraper to get players from the ladder\n",
    "print(\"üöÄ Starting New Recruit ladder scraping with Selenium...\")\n",
    "selenium_players = get_players_with_selenium(headless=False, timeout=15)\n",
    "print(f\"‚úÖ Found {len(selenium_players)} players\")\n",
    "\n",
    "if selenium_players:\n",
    "    print(\"\\nüìã First few players:\")\n",
    "    for i, player in enumerate(selenium_players[:5]):\n",
    "        print(f\"  {i+1}. {player['player_name']} (ID: {player['player_id']})\")\n",
    "        print(f\"     URL: {player['profile_url']}\")\n",
    "else:\n",
    "    print(\"‚ùå No players found. Let's debug the issue...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test to see the page content\n",
    "response = make_request(LADDER_URL)\n",
    "if response:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    page_text = soup.get_text()\n",
    "    \n",
    "    print(f\"Page length: {len(page_text)} characters\")\n",
    "    print(f\"Response status: {response.status_code}\")\n",
    "    print(f\"Content type: {response.headers.get('content-type', 'unknown')}\")\n",
    "    \n",
    "    # Check if this is a single-page application that loads content dynamically\n",
    "    if 'vue' in page_text.lower() or 'react' in page_text.lower() or len(page_text) < 1000:\n",
    "        print(\"\\\\nThis appears to be a single-page application with dynamic content loading\")\n",
    "        print(\"We may need to use different approach or find API endpoints\")\n",
    "    \n",
    "    # Look for any script tags that might indicate API endpoints\n",
    "    scripts = soup.find_all('script')\n",
    "    print(f\"\\\\nFound {len(scripts)} script tags\")\n",
    "    \n",
    "    # Look for any obvious game system identifiers\n",
    "    if 'old world' in page_text.lower():\n",
    "        print(\"\\\\n‚úÖ Found 'Old World' reference in page\")\n",
    "    else:\n",
    "        print(\"\\\\n‚ùå No 'Old World' reference found\")\n",
    "        \n",
    "    # Save the raw HTML for inspection\n",
    "    with open('ladder_page.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    print(\"\\\\nSaved raw HTML to 'ladder_page.html' for inspection\")\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to fetch ladder page\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: Try to find API endpoints or different URL patterns\n",
    "def explore_newrecruit_structure(session=None):\n",
    "    \"\"\"Explore the New Recruit site structure to find the right endpoints.\"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    \n",
    "    # Try common URL patterns that might have game-specific data\n",
    "    test_urls = [\n",
    "        f\"{BASE_URL}/ladder\",\n",
    "        f\"{BASE_URL}/ladder/tow\",  # The Old World abbreviation\n",
    "        f\"{BASE_URL}/ladder/warhammer-the-old-world\",\n",
    "        f\"{BASE_URL}/api/ladder\",\n",
    "        f\"{BASE_URL}/api/players\",\n",
    "        f\"{BASE_URL}/api/games\",\n",
    "        f\"{BASE_URL}/app/MySystems\",  # From the screenshot URL\n",
    "    ]\n",
    "    \n",
    "    for url in test_urls:\n",
    "        print(f\"\\nTesting: {url}\")\n",
    "        response = make_request(url, session)\n",
    "        if response and response.status_code == 200:\n",
    "            content_length = len(response.text)\n",
    "            content_type = response.headers.get('content-type', 'unknown')\n",
    "            print(f\"  ‚úÖ Success! Length: {content_length}, Type: {content_type}\")\n",
    "            \n",
    "            # Check if it contains useful data\n",
    "            if 'json' in content_type:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                    print(f\"  üìä JSON data with {len(data) if isinstance(data, (list, dict)) else 'unknown'} items\")\n",
    "                    if isinstance(data, dict) and 'players' in str(data).lower():\n",
    "                        print(\"  üéØ Contains player data!\")\n",
    "                except:\n",
    "                    print(\"  ‚ö†Ô∏è Invalid JSON\")\n",
    "            elif 'old world' in response.text.lower():\n",
    "                print(\"  üéØ Contains Old World reference!\")\n",
    "        else:\n",
    "            status = response.status_code if response else 'No response'\n",
    "            print(f\"  ‚ùå Failed: {status}\")\n",
    "\n",
    "# Run the exploration\n",
    "explore_newrecruit_structure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we know from the tournament reports that there are lists with specific IDs,\n",
    "# let's try to directly access some known army list URLs to understand the pattern\n",
    "def test_known_army_lists():\n",
    "    \"\"\"Test parsing known army list URLs from tournament reports.\"\"\"\n",
    "    \n",
    "    # Known URLs from our research\n",
    "    known_list_urls = [\n",
    "        \"https://www.newrecruit.eu/app/list/Wfirs\",  # Empire list from Woehammer\n",
    "        # We can add more as we find them\n",
    "    ]\n",
    "    \n",
    "    for list_url in known_list_urls:\n",
    "        print(f\"\\n=== TESTING KNOWN LIST: {list_url} ===\")\n",
    "        response = make_request(list_url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Try to extract basic info\n",
    "            title = soup.find('title')\n",
    "            if title:\n",
    "                print(f\"Title: {title.get_text()}\")\n",
    "                \n",
    "            # Look for faction and points info\n",
    "            page_text = soup.get_text()\n",
    "            \n",
    "            # Look for point values\n",
    "            points_matches = re.findall(r'\\[(\\d+)pts\\]', page_text)\n",
    "            if points_matches:\n",
    "                print(f\"Found point values: {points_matches[:5]}\")  # First 5\n",
    "                \n",
    "            # Look for faction names\n",
    "            if 'Empire' in page_text:\n",
    "                print(\"‚úÖ Contains Empire faction\")\n",
    "            if 'Old World' in page_text:\n",
    "                print(\"‚úÖ Contains Old World reference\")\n",
    "                \n",
    "            # Save this list for detailed analysis\n",
    "            filename = f\"list_{list_url.split('/')[-1]}.html\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"Saved to {filename}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ùå Failed to fetch {list_url}\")\n",
    "\n",
    "test_known_army_lists()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated approach: Since the main ladder is dynamic, let's try browser automation\n",
    "# First, let's see if we can detect what we need for Selenium\n",
    "\n",
    "def check_selenium_requirements():\n",
    "    \"\"\"Check if we can use Selenium for browser automation.\"\"\"\n",
    "    try:\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        print(\"‚úÖ Selenium is available\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Selenium not available: {e}\")\n",
    "        print(\"To install: pip install selenium\")\n",
    "        return False\n",
    "\n",
    "# Alternative: Parse the page source more carefully for hidden data\n",
    "def extract_vue_data_from_page():\n",
    "    \"\"\"Try to extract Vue.js data from the initial page load.\"\"\"\n",
    "    response = make_request(LADDER_URL)\n",
    "    if not response:\n",
    "        return None\n",
    "        \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Look for script tags that might contain initial data\n",
    "    scripts = soup.find_all('script')\n",
    "    \n",
    "    for script in scripts:\n",
    "        script_content = script.string if script.string else \"\"\n",
    "        \n",
    "        # Look for common patterns in Vue apps\n",
    "        if any(pattern in script_content for pattern in ['window.__INITIAL_STATE__', 'window.config', 'gameSystem', 'players']):\n",
    "            print(\"Found potential data in script tag!\")\n",
    "            print(script_content[:500])  # First 500 chars\n",
    "            return script_content\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"Checking requirements and looking for embedded data...\")\n",
    "selenium_available = check_selenium_requirements()\n",
    "vue_data = extract_vue_data_from_page()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium-based approach for dynamic content\n",
    "def get_players_with_selenium(headless=True, timeout=10):\n",
    "    \"\"\"Use Selenium to interact with the dynamic ladder page.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        \n",
    "        # Set up Chrome options\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        \n",
    "        # Create driver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        try:\n",
    "            print(\"üöÄ Opening New Recruit ladder page...\")\n",
    "            driver.get(LADDER_URL)\n",
    "            \n",
    "            # Wait for page to load\n",
    "            wait = WebDriverWait(driver, timeout)\n",
    "            \n",
    "            # Look for the game system dropdown\n",
    "            print(\"üîç Looking for game system dropdown...\")\n",
    "            try:\n",
    "                # Try to find dropdown with \"Warhammer The Old World\" option\n",
    "                dropdown_selectors = [\n",
    "                    \"select\",  # Generic select element\n",
    "                    \"[data-v-5065dd38]\",  # From user's HTML snippet\n",
    "                    \".select\",\n",
    "                    \".dropdown\"\n",
    "                ]\n",
    "                \n",
    "                dropdown = None\n",
    "                for selector in dropdown_selectors:\n",
    "                    try:\n",
    "                        dropdown = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
    "                        print(f\"‚úÖ Found dropdown with selector: {selector}\")\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if dropdown:\n",
    "                    # Try to select \"Warhammer The Old World\"\n",
    "                    select = Select(dropdown)\n",
    "                    \n",
    "                    # Print all available options\n",
    "                    options = select.options\n",
    "                    print(f\"üìã Found {len(options)} dropdown options:\")\n",
    "                    for i, option in enumerate(options):\n",
    "                        option_text = option.get_attribute('textContent') or option.text\n",
    "                        print(f\"  {i}: {option_text}\")\n",
    "                        \n",
    "                        # Try to select Old World option\n",
    "                        if 'old world' in option_text.lower():\n",
    "                            print(f\"üéØ Selecting: {option_text}\")\n",
    "                            select.select_by_visible_text(option_text)\n",
    "                            time.sleep(2)  # Wait for content to load\n",
    "                            break\n",
    "                \n",
    "                # Now look for player links\n",
    "                print(\"üîç Looking for player profile links...\")\n",
    "                player_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='Profile']\")\n",
    "                \n",
    "                players = []\n",
    "                for link in player_links:\n",
    "                    href = link.get_attribute('href')\n",
    "                    text = link.text.strip()\n",
    "                    if href and text and 'id=' in href:\n",
    "                        player_id = extract_id_from_url(href)\n",
    "                        if player_id:\n",
    "                            players.append({\n",
    "                                'player_id': player_id,\n",
    "                                'player_name': text,\n",
    "                                'profile_url': href\n",
    "                            })\n",
    "                \n",
    "                print(f\"‚úÖ Found {len(players)} players!\")\n",
    "                return players\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error finding dropdown or players: {e}\")\n",
    "                \n",
    "                # Fallback: Save page source for debugging\n",
    "                with open('selenium_page_source.html', 'w', encoding='utf-8') as f:\n",
    "                    f.write(driver.page_source)\n",
    "                print(\"üíæ Saved page source to selenium_page_source.html\")\n",
    "                \n",
    "                return []\n",
    "        \n",
    "        finally:\n",
    "            driver.quit()\n",
    "            \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Selenium not available: {e}\")\n",
    "        print(\"Install with: pip install selenium webdriver-manager\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Selenium error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test the selenium approach\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"TESTING SELENIUM APPROACH\")\n",
    "print(\"=\"*50)\n",
    "selenium_players = get_players_with_selenium(headless=False)  # Set to False to see the browser\n",
    "print(f\"Result: {len(selenium_players)} players found\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Main Scraping Logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main scraping function defined!\n"
     ]
    }
   ],
   "source": [
    "def scrape_all_army_lists(max_players=10, max_matches_per_player=5):\n",
    "    \"\"\"\n",
    "    Main function to scrape army lists from New Recruit ladder.\n",
    "    \n",
    "    Args:\n",
    "        max_players: Maximum number of players to scrape (for testing)\n",
    "        max_matches_per_player: Maximum matches to check per player\n",
    "    \n",
    "    Returns:\n",
    "        List of army list data dictionaries\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    all_army_data = []\n",
    "    \n",
    "    logger.info(\"Starting army list scraping...\")\n",
    "    \n",
    "    # Step 1: Get player IDs from ladder\n",
    "    players = get_player_ids_from_ladder(session)\n",
    "    if not players:\n",
    "        logger.error(\"No players found on ladder!\")\n",
    "        return []\n",
    "    \n",
    "    logger.info(f\"Found {len(players)} players, processing first {max_players}\")\n",
    "    \n",
    "    # Step 2: For each player, get their match history\n",
    "    for i, player in enumerate(players[:max_players]):\n",
    "        logger.info(f\"Processing player {i+1}/{min(len(players), max_players)}: {player['player_name']}\")\n",
    "        \n",
    "        # Get match URLs from player profile\n",
    "        match_urls = get_match_history_urls(player['profile_url'], session)\n",
    "        \n",
    "        # Step 3: For each match, extract army list URLs\n",
    "        for j, match_url in enumerate(match_urls[:max_matches_per_player]):\n",
    "            logger.info(f\"  Processing match {j+1}/{min(len(match_urls), max_matches_per_player)}\")\n",
    "            \n",
    "            match_data = extract_army_list_urls_from_match(match_url, session)\n",
    "            \n",
    "            # Step 4: Parse player's army list\n",
    "            if match_data.get('player_list_url'):\n",
    "                army_data = parse_army_list(match_data['player_list_url'], session)\n",
    "                if army_data:\n",
    "                    # Add match context\n",
    "                    army_data.update({\n",
    "                        'player_name': player['player_name'],\n",
    "                        'player_id': player['player_id'],\n",
    "                        'match_url': match_url,\n",
    "                        'match_result': match_data.get('match_result'),\n",
    "                        'opponent_list_url': match_data.get('opponent_list_url')\n",
    "                    })\n",
    "                    all_army_data.append(army_data)\n",
    "            \n",
    "            # Optional: Also parse opponent's army list for matchup data\n",
    "            if match_data.get('opponent_list_url'):\n",
    "                opponent_army_data = parse_army_list(match_data['opponent_list_url'], session)\n",
    "                if opponent_army_data:\n",
    "                    opponent_army_data.update({\n",
    "                        'player_name': 'Opponent',  # We might not have opponent name easily\n",
    "                        'match_url': match_url,\n",
    "                        'is_opponent': True\n",
    "                    })\n",
    "                    all_army_data.append(opponent_army_data)\n",
    "    \n",
    "    logger.info(f\"Scraping complete! Collected {len(all_army_data)} army lists\")\n",
    "    return all_army_data\n",
    "\n",
    "print(\"Main scraping function defined!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Data Analysis and Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data analysis and export functions defined!\n"
     ]
    }
   ],
   "source": [
    "def analyze_faction_performance(army_data):\n",
    "    \"\"\"Analyze performance by faction to identify the best lists.\"\"\"\n",
    "    faction_stats = {}\n",
    "    \n",
    "    for army in army_data:\n",
    "        faction = army.get('faction', 'Unknown')\n",
    "        if faction not in faction_stats:\n",
    "            faction_stats[faction] = {\n",
    "                'total_lists': 0,\n",
    "                'wins': 0,\n",
    "                'losses': 0,\n",
    "                'win_rate': 0,\n",
    "                'sample_lists': []\n",
    "            }\n",
    "        \n",
    "        faction_stats[faction]['total_lists'] += 1\n",
    "        \n",
    "        # Analyze match results (this depends on how we capture win/loss data)\n",
    "        result = army.get('match_result', '').lower()\n",
    "        if 'win' in result or 'victory' in result:\n",
    "            faction_stats[faction]['wins'] += 1\n",
    "        elif 'loss' in result or 'defeat' in result:\n",
    "            faction_stats[faction]['losses'] += 1\n",
    "        \n",
    "        # Store sample list for this faction\n",
    "        if len(faction_stats[faction]['sample_lists']) < 3:\n",
    "            faction_stats[faction]['sample_lists'].append({\n",
    "                'list_url': army.get('list_url'),\n",
    "                'player_name': army.get('player_name'),\n",
    "                'total_points': army.get('total_points')\n",
    "            })\n",
    "    \n",
    "    # Calculate win rates\n",
    "    for faction, stats in faction_stats.items():\n",
    "        total_games = stats['wins'] + stats['losses']\n",
    "        if total_games > 0:\n",
    "            stats['win_rate'] = stats['wins'] / total_games\n",
    "    \n",
    "    return faction_stats\n",
    "\n",
    "def save_data_to_files(army_data, base_filename='warhammer_old_world_data'):\n",
    "    \"\"\"Save scraped data to multiple formats.\"\"\"\n",
    "    \n",
    "    # Save raw data as JSON\n",
    "    json_filename = f\"{base_filename}.json\"\n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(army_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Raw data saved to {json_filename}\")\n",
    "    \n",
    "    # Save as CSV for easy analysis\n",
    "    csv_filename = f\"{base_filename}.csv\"\n",
    "    \n",
    "    # Flatten the data for CSV\n",
    "    flattened_data = []\n",
    "    for army in army_data:\n",
    "        flat_army = {\n",
    "            'faction': army.get('faction', ''),\n",
    "            'list_name': army.get('list_name', ''),\n",
    "            'total_points': army.get('total_points', 0),\n",
    "            'player_name': army.get('player_name', ''),\n",
    "            'player_id': army.get('player_id', ''),\n",
    "            'match_result': army.get('match_result', ''),\n",
    "            'list_url': army.get('list_url', ''),\n",
    "            'match_url': army.get('match_url', ''),\n",
    "            'opponent_list_url': army.get('opponent_list_url', ''),\n",
    "            'characters_count': len(army.get('characters', [])),\n",
    "            'core_units_count': len(army.get('core_units', [])),\n",
    "            'special_units_count': len(army.get('special_units', [])),\n",
    "            'rare_units_count': len(army.get('rare_units', []))\n",
    "        }\n",
    "        flattened_data.append(flat_army)\n",
    "    \n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"CSV data saved to {csv_filename}\")\n",
    "    \n",
    "    # Save faction analysis\n",
    "    faction_stats = analyze_faction_performance(army_data)\n",
    "    faction_filename = f\"{base_filename}_faction_analysis.json\"\n",
    "    with open(faction_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(faction_stats, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Faction analysis saved to {faction_filename}\")\n",
    "    \n",
    "    return df, faction_stats\n",
    "\n",
    "print(\"Data analysis and export functions defined!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Test and Execute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "WARHAMMER THE OLD WORLD ARMY LIST SCRAPER\n",
      "==================================================\n",
      "\n",
      "This scraper will:\n",
      "1. Access the New Recruit ladder for Warhammer The Old World\n",
      "2. Extract player profiles and match histories\n",
      "3. Parse army lists from match details\n",
      "4. Analyze faction performance\n",
      "5. Export data for machine learning and analysis\n",
      "\n",
      "Before running the full scraper, test individual functions:\n",
      "- Run get_player_ids_from_ladder() to test ladder access\n",
      "- Test a single player profile with get_match_history_urls()\n",
      "- Parse a known army list URL with parse_army_list()\n",
      "\n",
      "To run the full scraper, call:\n",
      "army_data = scrape_all_army_lists(max_players=5, max_matches_per_player=3)\n",
      "df, faction_stats = save_data_to_files(army_data)\n",
      "\n",
      "READY TO START SCRAPING!\n"
     ]
    }
   ],
   "source": [
    "# Test the scraper with a small sample first\n",
    "print(\"=\"*50)\n",
    "print(\"WARHAMMER THE OLD WORLD ARMY LIST SCRAPER\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "print(\"This scraper will:\")\n",
    "print(\"1. Access the New Recruit ladder for Warhammer The Old World\")\n",
    "print(\"2. Extract player profiles and match histories\")  \n",
    "print(\"3. Parse army lists from match details\")\n",
    "print(\"4. Analyze faction performance\")\n",
    "print(\"5. Export data for machine learning and analysis\")\n",
    "print()\n",
    "print(\"Before running the full scraper, test individual functions:\")\n",
    "print(\"- Run get_player_ids_from_ladder() to test ladder access\")\n",
    "print(\"- Test a single player profile with get_match_history_urls()\")\n",
    "print(\"- Parse a known army list URL with parse_army_list()\")\n",
    "print()\n",
    "print(\"To run the full scraper, call:\")\n",
    "print(\"army_data = scrape_all_army_lists(max_players=5, max_matches_per_player=3)\")\n",
    "print(\"df, faction_stats = save_data_to_files(army_data)\")\n",
    "print()\n",
    "print(\"READY TO START SCRAPING!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Fetching player IDs from ladder...\n",
      "INFO:__main__:Found 0 players\n"
     ]
    }
   ],
   "source": [
    "players = get_player_ids_from_ladder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
